{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping abstract information\n",
    "March 4, 2018\n",
    "This notebook scrapes abstract text from:\n",
    "- Proceedings of the Annual Cognitive Science Society meeting archive (html)\n",
    "- Proceedings of Cognitive Neuroscience Society annual meeting (text converted from pdf)\n",
    "\n",
    "Abstracts are then stored in a spreadsheet, containing information such as year, authors, title, and abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_CS(home_url, data_file):\n",
    "    #connect to home page url for that year\n",
    "    CSurl = urllib.request.urlopen(home_url).read()\n",
    "    soup = BeautifulSoup(CSurl, 'html.parser')\n",
    "    all_links = soup.find_all('a', attrs={'href': re.compile(\"papers/*\")})    \n",
    "    year = home_url[-5:-1]    \n",
    "    \n",
    "    # enumerate through all paper links\n",
    "    for link_idx, link in enumerate(all_links):\n",
    "        # get soup from paper url\n",
    "        if home_url not in str(link['href']):\n",
    "            url_text = home_url + str(link['href'])\n",
    "        else:\n",
    "            url_text = str(link['href'])\n",
    "    \n",
    "        url = urllib.request.urlopen(url_text).read()\n",
    "        soup = BeautifulSoup(url, 'html.parser')\n",
    "    \n",
    "        # scrape & parse\n",
    "        authors = []\n",
    "        affl = []\n",
    "        title = ' '.join(soup.find_all('h1')[0].text.split())\n",
    "        # exception rule for 2014 abstracts\n",
    "        if '2014' in home_url:            \n",
    "            abstr = ' '.join(soup.find_all('blockquote')[1].text.split())\n",
    "        else:            \n",
    "            abstr = ' '.join(soup.find_all('p', {\"id\": \"abstract\"})[0].text.split())            \n",
    "        \n",
    "        soup.find_all('ul')\n",
    "        for ana in soup.find_all('em'):\n",
    "            affl.append('>'+ana.text)\n",
    "            if '2014' in home_url:\n",
    "                # somebody fucked something up in 2014\n",
    "                authors.append('>' + ana.previous_element.previous_element.split(',')[0])\n",
    "            else:            \n",
    "                authors.append('>' + ana.previous_element.split(',')[0])\n",
    "        \n",
    "        # do some gymnastics to get it into a pandas df and add as a row to CSV\n",
    "        new_row = {'Year': str(year), 'Title': title,'Abstract': abstr,'Authors': ''.join(authors),'Affiliations': ''.join(affl), 'URL': url_text}\n",
    "        df_cur = pd.Series(data=new_row).to_frame().T[['Year','Title','Abstract','Authors','Affiliations','URL']]\n",
    "        df_cur.to_csv(data_file, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather CNS abstracts from text to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_folder = '../data/COSYNE_programs/'\n",
    "# os.listdir(data_folder)\n",
    "# CNS_files = sorted([f for f in os.listdir(data_folder) if ('Cosyne' in f) and ('.txt' in f)])#[:-1]#not include 2017 forget why though\n",
    "# print(CNS_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not include 2008 and 2009 as those years are not consistent with formatting.\n",
    "\n",
    "The years parsed are purely abstracts however I can add a few lines I think and include the talks. \n",
    "    The formatting seemed semi consistent to grab the talks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cosyne2010-programme.txt', 'Cosyne2012_program_book.txt', 'Cosyne2013_program_book.txt', 'Cosyne2014_program_book.txt', 'Cosyne2015_program_book.txt', 'Cosyne2016_program_book.txt', 'Cosyne2017_program_book.txt', 'Cosyne2018_program_book.txt']\n"
     ]
    }
   ],
   "source": [
    "data_folder = '../data/COSYNE_programs/'\n",
    "os.listdir(data_folder)\n",
    "CNS_files = sorted([f for f in os.listdir(data_folder) if ('Cosyne' in f) and ('.txt' in f)])[2:]#not include 08/09\n",
    "print(CNS_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosyne2010-programme.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "Cosyne2010-programme.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "j\n",
      "Cosyne2012_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "Cosyne2012_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "j\n",
      "Cosyne2013_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Cosyne2013_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "j\n",
      "Cosyne2014_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Cosyne2014_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "j\n",
      "Cosyne2015_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "Cosyne2015_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "j\n",
      "Cosyne2016_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "Cosyne2016_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "j\n",
      "Cosyne2017_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "Cosyne2017_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "j\n",
      "Cosyne2018_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "Cosyne2018_program_book.txt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "j\n"
     ]
    }
   ],
   "source": [
    "abstracts=[]\n",
    "journal=[]\n",
    "\n",
    "let_vec = ['I-','II-','III-']\n",
    "\n",
    "for x in range(len(CNS_files)):\n",
    "\n",
    "    try:\n",
    "        file = open(data_folder+CNS_files[x], 'r')\n",
    "        data = file.read()\n",
    "        data_list = data.split('\\n')\n",
    "        abs_start = [ind for ind, d in enumerate(data_list) if '– I-1' in d][0]\n",
    "\n",
    "        print(CNS_files[x])\n",
    "        abs_list = data_list[abs_start:]\n",
    "\n",
    "\n",
    "        mush=' '.join(abs_list)\n",
    "\n",
    "\n",
    "        for j in range(0,len(let_vec)):\n",
    "\n",
    "            print(j)\n",
    "\n",
    "            for i in range(0,200):\n",
    "\n",
    "                try:\n",
    "                    cur_section= let_vec[j]\n",
    "                    add_dot = '.'\n",
    "                    cur_abs=i\n",
    "                    abs_beg_ind = mush.index(cur_section+'%i'%cur_abs+add_dot)\n",
    "                    abs_end_ind = mush.index(cur_section+'%i'%(cur_abs+1)+add_dot)\n",
    "                    last_index = cur_section+'%i'%(cur_abs+1)+add_dot\n",
    "                    print(cur_abs)\n",
    "\n",
    "                    section_abst = mush[abs_beg_ind:abs_end_ind]\n",
    "                    \n",
    "\n",
    "                    abstracts.append(section_abst)\n",
    "                    journal.append(CNS_files[x])\n",
    "\n",
    "                    #last_good = i\n",
    "                except:\n",
    "                    'urmom'\n",
    "    except:\n",
    "        file = open(data_folder+CNS_files[x], 'r')\n",
    "        data = file.read()\n",
    "        data_list = data.split('\\n')\n",
    "        abs_start = [ind for ind, d in enumerate(data_list) if 'I-1 –' in d][0]\n",
    "        print(CNS_files[x])\n",
    "\n",
    "        abs_list = data_list[abs_start:]\n",
    "\n",
    "\n",
    "        mush=' '.join(abs_list)\n",
    "        \n",
    "        for j in range(0,len(let_vec)):\n",
    "\n",
    "            print(j)\n",
    "\n",
    "            for i in range(0,200):\n",
    "\n",
    "                try:\n",
    "                    cur_section= let_vec[j]\n",
    "                    add_dot = '.'\n",
    "                    cur_abs=i\n",
    "                    abs_beg_ind = mush.index(cur_section+'%i'%cur_abs+add_dot)\n",
    "                    abs_end_ind = mush.index(cur_section+'%i'%(cur_abs+1)+add_dot)\n",
    "                    last_index = cur_section+'%i'%(cur_abs+1)+add_dot\n",
    "                    print(cur_abs)\n",
    "\n",
    "                    section_abst = mush[abs_beg_ind:abs_end_ind]\n",
    "\n",
    "                    abstracts.append(section_abst)\n",
    "                    journal.append(CNS_files[x])\n",
    "\n",
    "                    #last_good = i\n",
    "                except:\n",
    "                    'urmom'\n",
    "    finally:\n",
    "        file = open(data_folder+CNS_files[x], 'r')\n",
    "        data = file.read()\n",
    "        data_list = data.split('\\n')\n",
    "        abs_start = [ind for ind, d in enumerate(data_list) if '– I-1' in d][0]\n",
    "        print(CNS_files[x])\n",
    "\n",
    "        abs_list = data_list[abs_start:]\n",
    "\n",
    "\n",
    "        mush=' '.join(abs_list)\n",
    "        \n",
    "        for j in range(0,len(let_vec)):\n",
    "\n",
    "            print(j)\n",
    "\n",
    "            for i in range(0,200):\n",
    "\n",
    "                try:\n",
    "                    cur_section= let_vec[j]\n",
    "                    add_dot = '.'\n",
    "                    cur_abs=i\n",
    "                    abs_beg_ind = mush.index(cur_section+'%i'%cur_abs+add_dot)\n",
    "                    abs_end_ind = mush.index(cur_section+'%i'%(cur_abs+1)+add_dot)\n",
    "                    last_index = cur_section+'%i'%(cur_abs+1)+add_dot\n",
    "                    print(cur_abs)\n",
    "\n",
    "                    section_abst = mush[abs_beg_ind:abs_end_ind]\n",
    "\n",
    "                    abstracts.append(section_abst)\n",
    "                    journal.append(CNS_files[x])\n",
    "\n",
    "                    #last_good = i\n",
    "                except:\n",
    "                    'urmom'\n",
    "    \n",
    "    print('j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstracts</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-1. Intrinsic dendritic plasticity maximally ...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-2. Analytical study of history dependent tim...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-3. Fast Kalman filtering on quasilinear dend...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-4. Dendritic spine plasticity can stabilize ...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-5. Model of synaptic plasticity based on sel...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-6. Trajectory prediction combining forward m...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-7. Dynamics of fronto-parietal synchrony dur...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-8. Bayesian optimal use of visual feature cu...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-9. Neural correlates of spatial short-term m...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-10. When to recall a memory? Epoch dependent...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-11. Neural encoding of decision uncertainty ...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-12. Clocking perceptual processing speed: fr...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-13. The effect of time pressure on decision ...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-14. Decision-related activity in area V2 for...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-15. Changes in functional connectivity in LI...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I-16. Role of secondary motor cortex in withho...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I-17. Dorsomedial prefrontal cortex encodes va...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-18. Striatal activity consistent with model-...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I-19. From integrate-and-fire neurons to Gener...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I-20. Excitatory-inhibitory correlations resul...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I-21. Salience and surround interactions via n...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I-22. A three-layer model of natural image sta...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I-23. Learning Lp spherical potentials for Mar...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I-24. Grasping image statistics Omid Aladini C...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I-25. Beyond magical numbers: towards a noise-...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I-26. Identifiability of nonlinear receptive f...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I-27. Detecting a change by a single neuron Hi...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I-28. Complexity and performance in simple neu...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I-29. Short-term synaptic plasticity and senso...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I-30. The speed of time Misha Ahrens1,2 Manees...</td>\n",
       "      <td>Cosyne2010-programme.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>III-92. Empirical vine copula modeling to stud...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>III-93. Cortical mechanisms for robust sensory...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>III-94. Efficient coding in V1: Oriented filte...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>III-95. A 3rd factor w/o a 2nd: Dopamine and p...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>III-96. Neuron dendrograms uncover asymmetrica...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>III-97. A biologically inspired neural network...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>III-98. A Bayesian psychophysics model of sens...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>III-99. Using multiple optimization tasks to i...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>III-100. In the footsteps of learning: changes...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5007</th>\n",
       "      <td>III-101. A recurrent neural network model for ...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>III-102. A theory of memory replay and general...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>III-103. Understanding camouflage detection Ab...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>III-104. Stability of hippocampal spiking sequ...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>III-105. Spike inference for genetically encod...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>III-106. Deep neuronal networks with recursive...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>III-107. A visual projection neuron class stop...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>III-108. A spatiotemporally-resolved view of c...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>III-109. Representation of choice bias in the ...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>III-110. Manifold inference from neural dynami...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>III-111. Functional investigation of behaviora...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>III-112. Systems consolidation without replay?...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5019</th>\n",
       "      <td>III-113. An integrated hierarchical control ar...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>III-114. Nonlinear impact of structural plasti...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>III-115. A minimal model for coherent chaos in...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>III-116. Only a subset of asynchronous irregul...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>III-117. Sensory cortex is optimised for predi...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>III-118. Mixed selectivity and population codi...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>III-119. Controlling burst activity in cortica...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>III-120. Understanding functional clusters in ...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>III-121. Gradient descent for spiking neural n...</td>\n",
       "      <td>Cosyne2018_program_book.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5028 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstracts  \\\n",
       "0     I-1. Intrinsic dendritic plasticity maximally ...   \n",
       "1     I-2. Analytical study of history dependent tim...   \n",
       "2     I-3. Fast Kalman filtering on quasilinear dend...   \n",
       "3     I-4. Dendritic spine plasticity can stabilize ...   \n",
       "4     I-5. Model of synaptic plasticity based on sel...   \n",
       "5     I-6. Trajectory prediction combining forward m...   \n",
       "6     I-7. Dynamics of fronto-parietal synchrony dur...   \n",
       "7     I-8. Bayesian optimal use of visual feature cu...   \n",
       "8     I-9. Neural correlates of spatial short-term m...   \n",
       "9     I-10. When to recall a memory? Epoch dependent...   \n",
       "10    I-11. Neural encoding of decision uncertainty ...   \n",
       "11    I-12. Clocking perceptual processing speed: fr...   \n",
       "12    I-13. The effect of time pressure on decision ...   \n",
       "13    I-14. Decision-related activity in area V2 for...   \n",
       "14    I-15. Changes in functional connectivity in LI...   \n",
       "15    I-16. Role of secondary motor cortex in withho...   \n",
       "16    I-17. Dorsomedial prefrontal cortex encodes va...   \n",
       "17    I-18. Striatal activity consistent with model-...   \n",
       "18    I-19. From integrate-and-fire neurons to Gener...   \n",
       "19    I-20. Excitatory-inhibitory correlations resul...   \n",
       "20    I-21. Salience and surround interactions via n...   \n",
       "21    I-22. A three-layer model of natural image sta...   \n",
       "22    I-23. Learning Lp spherical potentials for Mar...   \n",
       "23    I-24. Grasping image statistics Omid Aladini C...   \n",
       "24    I-25. Beyond magical numbers: towards a noise-...   \n",
       "25    I-26. Identifiability of nonlinear receptive f...   \n",
       "26    I-27. Detecting a change by a single neuron Hi...   \n",
       "27    I-28. Complexity and performance in simple neu...   \n",
       "28    I-29. Short-term synaptic plasticity and senso...   \n",
       "29    I-30. The speed of time Misha Ahrens1,2 Manees...   \n",
       "...                                                 ...   \n",
       "4998  III-92. Empirical vine copula modeling to stud...   \n",
       "4999  III-93. Cortical mechanisms for robust sensory...   \n",
       "5000  III-94. Efficient coding in V1: Oriented filte...   \n",
       "5001  III-95. A 3rd factor w/o a 2nd: Dopamine and p...   \n",
       "5002  III-96. Neuron dendrograms uncover asymmetrica...   \n",
       "5003  III-97. A biologically inspired neural network...   \n",
       "5004  III-98. A Bayesian psychophysics model of sens...   \n",
       "5005  III-99. Using multiple optimization tasks to i...   \n",
       "5006  III-100. In the footsteps of learning: changes...   \n",
       "5007  III-101. A recurrent neural network model for ...   \n",
       "5008  III-102. A theory of memory replay and general...   \n",
       "5009  III-103. Understanding camouflage detection Ab...   \n",
       "5010  III-104. Stability of hippocampal spiking sequ...   \n",
       "5011  III-105. Spike inference for genetically encod...   \n",
       "5012  III-106. Deep neuronal networks with recursive...   \n",
       "5013  III-107. A visual projection neuron class stop...   \n",
       "5014  III-108. A spatiotemporally-resolved view of c...   \n",
       "5015  III-109. Representation of choice bias in the ...   \n",
       "5016  III-110. Manifold inference from neural dynami...   \n",
       "5017  III-111. Functional investigation of behaviora...   \n",
       "5018  III-112. Systems consolidation without replay?...   \n",
       "5019  III-113. An integrated hierarchical control ar...   \n",
       "5020  III-114. Nonlinear impact of structural plasti...   \n",
       "5021  III-115. A minimal model for coherent chaos in...   \n",
       "5022  III-116. Only a subset of asynchronous irregul...   \n",
       "5023  III-117. Sensory cortex is optimised for predi...   \n",
       "5024  III-118. Mixed selectivity and population codi...   \n",
       "5025  III-119. Controlling burst activity in cortica...   \n",
       "5026  III-120. Understanding functional clusters in ...   \n",
       "5027  III-121. Gradient descent for spiking neural n...   \n",
       "\n",
       "                          journal  \n",
       "0        Cosyne2010-programme.txt  \n",
       "1        Cosyne2010-programme.txt  \n",
       "2        Cosyne2010-programme.txt  \n",
       "3        Cosyne2010-programme.txt  \n",
       "4        Cosyne2010-programme.txt  \n",
       "5        Cosyne2010-programme.txt  \n",
       "6        Cosyne2010-programme.txt  \n",
       "7        Cosyne2010-programme.txt  \n",
       "8        Cosyne2010-programme.txt  \n",
       "9        Cosyne2010-programme.txt  \n",
       "10       Cosyne2010-programme.txt  \n",
       "11       Cosyne2010-programme.txt  \n",
       "12       Cosyne2010-programme.txt  \n",
       "13       Cosyne2010-programme.txt  \n",
       "14       Cosyne2010-programme.txt  \n",
       "15       Cosyne2010-programme.txt  \n",
       "16       Cosyne2010-programme.txt  \n",
       "17       Cosyne2010-programme.txt  \n",
       "18       Cosyne2010-programme.txt  \n",
       "19       Cosyne2010-programme.txt  \n",
       "20       Cosyne2010-programme.txt  \n",
       "21       Cosyne2010-programme.txt  \n",
       "22       Cosyne2010-programme.txt  \n",
       "23       Cosyne2010-programme.txt  \n",
       "24       Cosyne2010-programme.txt  \n",
       "25       Cosyne2010-programme.txt  \n",
       "26       Cosyne2010-programme.txt  \n",
       "27       Cosyne2010-programme.txt  \n",
       "28       Cosyne2010-programme.txt  \n",
       "29       Cosyne2010-programme.txt  \n",
       "...                           ...  \n",
       "4998  Cosyne2018_program_book.txt  \n",
       "4999  Cosyne2018_program_book.txt  \n",
       "5000  Cosyne2018_program_book.txt  \n",
       "5001  Cosyne2018_program_book.txt  \n",
       "5002  Cosyne2018_program_book.txt  \n",
       "5003  Cosyne2018_program_book.txt  \n",
       "5004  Cosyne2018_program_book.txt  \n",
       "5005  Cosyne2018_program_book.txt  \n",
       "5006  Cosyne2018_program_book.txt  \n",
       "5007  Cosyne2018_program_book.txt  \n",
       "5008  Cosyne2018_program_book.txt  \n",
       "5009  Cosyne2018_program_book.txt  \n",
       "5010  Cosyne2018_program_book.txt  \n",
       "5011  Cosyne2018_program_book.txt  \n",
       "5012  Cosyne2018_program_book.txt  \n",
       "5013  Cosyne2018_program_book.txt  \n",
       "5014  Cosyne2018_program_book.txt  \n",
       "5015  Cosyne2018_program_book.txt  \n",
       "5016  Cosyne2018_program_book.txt  \n",
       "5017  Cosyne2018_program_book.txt  \n",
       "5018  Cosyne2018_program_book.txt  \n",
       "5019  Cosyne2018_program_book.txt  \n",
       "5020  Cosyne2018_program_book.txt  \n",
       "5021  Cosyne2018_program_book.txt  \n",
       "5022  Cosyne2018_program_book.txt  \n",
       "5023  Cosyne2018_program_book.txt  \n",
       "5024  Cosyne2018_program_book.txt  \n",
       "5025  Cosyne2018_program_book.txt  \n",
       "5026  Cosyne2018_program_book.txt  \n",
       "5027  Cosyne2018_program_book.txt  \n",
       "\n",
       "[5028 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_table=pd.DataFrame({'journal':journal,'abstracts':abstracts})\n",
    "try_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try_table.to_csv(\"COSYNE_SCRAPED_DI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=try_table.abstracts[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehh=test.split(' ')[::-1]\n",
    "ehh_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " 'doi:',\n",
       " 'cells.',\n",
       " 'place',\n",
       " 'real',\n",
       " 'of',\n",
       " 'selectivity',\n",
       " 'and',\n",
       " 'robustness',\n",
       " 'the',\n",
       " 'mimicking',\n",
       " 'responses,',\n",
       " 'new',\n",
       " 'acquiring',\n",
       " 'avoid',\n",
       " 'and',\n",
       " 'representations',\n",
       " 'encoded',\n",
       " 'previously',\n",
       " 'reactivate',\n",
       " 'will',\n",
       " 'environment',\n",
       " 'learned',\n",
       " 'previously',\n",
       " 'a',\n",
       " 'through',\n",
       " 'traversing',\n",
       " 'whether',\n",
       " 'test',\n",
       " 'to',\n",
       " 'is',\n",
       " 'goal',\n",
       " 'Our',\n",
       " 'cues.',\n",
       " 'visual',\n",
       " 'certain',\n",
       " 'to',\n",
       " 'preferentially',\n",
       " 'respond',\n",
       " 'to',\n",
       " 'tuned',\n",
       " 'cells',\n",
       " 'and',\n",
       " 'cells',\n",
       " 'grid',\n",
       " 'mimicking',\n",
       " 'units',\n",
       " 'spiking',\n",
       " 'into',\n",
       " 'processed',\n",
       " 'is',\n",
       " 'which',\n",
       " 'footage,',\n",
       " 'video',\n",
       " 'from',\n",
       " 'derived',\n",
       " 'data',\n",
       " 'real-world',\n",
       " 'with',\n",
       " 'model',\n",
       " 'this',\n",
       " 'testing',\n",
       " 'currently',\n",
       " 'are',\n",
       " 'We',\n",
       " 'basis.',\n",
       " 'pattern-by-pattern',\n",
       " 'a',\n",
       " 'on',\n",
       " 'DG',\n",
       " 'and',\n",
       " 'CA3',\n",
       " 'between',\n",
       " 'timing',\n",
       " 'spike',\n",
       " 'relative',\n",
       " 'of',\n",
       " 'virtue',\n",
       " 'by',\n",
       " 'known',\n",
       " 'or',\n",
       " 'novel',\n",
       " 'as',\n",
       " 'recognized',\n",
       " 'be',\n",
       " 'can',\n",
       " 'patterns',\n",
       " 'input',\n",
       " 'interneurons)',\n",
       " 'inhibitory',\n",
       " 'some',\n",
       " 'of',\n",
       " 'addition',\n",
       " 'the',\n",
       " '(with',\n",
       " 'then',\n",
       " 'group,',\n",
       " 'cell',\n",
       " 'a',\n",
       " 'instead',\n",
       " 'is',\n",
       " 'microcircuit',\n",
       " 'the',\n",
       " 'in',\n",
       " 'cells',\n",
       " 'three',\n",
       " 'the',\n",
       " 'of',\n",
       " 'each',\n",
       " 'if',\n",
       " 'that',\n",
       " 'demonstrate',\n",
       " 'We',\n",
       " 'hippocampus.',\n",
       " 'the',\n",
       " 'of',\n",
       " 'CA3',\n",
       " 'subregion',\n",
       " 'and',\n",
       " '(DG)',\n",
       " 'gyrus',\n",
       " 'dentate',\n",
       " 'the',\n",
       " '(EC),',\n",
       " 'cortex',\n",
       " 'entorhinal',\n",
       " 'the',\n",
       " 'of',\n",
       " 'that',\n",
       " 'to',\n",
       " 'similar',\n",
       " 'is',\n",
       " 'microcircuit',\n",
       " 'this',\n",
       " 'of',\n",
       " 'structure',\n",
       " 'The',\n",
       " 'input.',\n",
       " 'the',\n",
       " 'of',\n",
       " 'familiarity',\n",
       " 'the',\n",
       " 'signals',\n",
       " '-',\n",
       " 'cell',\n",
       " 'auxiliary',\n",
       " 'the',\n",
       " 'and',\n",
       " 'cell',\n",
       " 'target',\n",
       " 'the',\n",
       " 'between',\n",
       " 'timing',\n",
       " 'spike',\n",
       " 'relative',\n",
       " 'the',\n",
       " '-',\n",
       " 'race',\n",
       " 'this',\n",
       " 'of',\n",
       " 'result',\n",
       " 'The',\n",
       " 'pathway.',\n",
       " 'disynaptic',\n",
       " 'the',\n",
       " 'via',\n",
       " 'cell',\n",
       " 'same',\n",
       " 'the',\n",
       " 'of',\n",
       " 'activation',\n",
       " 'the',\n",
       " 'and',\n",
       " 'pathway',\n",
       " 'monosynaptic',\n",
       " 'the',\n",
       " 'via',\n",
       " 'cell',\n",
       " 'target',\n",
       " 'the',\n",
       " 'of',\n",
       " 'activation',\n",
       " 'between',\n",
       " 'exists',\n",
       " 'race',\n",
       " 'a',\n",
       " 'network,',\n",
       " 'this',\n",
       " 'in',\n",
       " 'that',\n",
       " 'demonstrate',\n",
       " 'We',\n",
       " 'synapses.',\n",
       " 'fixed-weight',\n",
       " 'with',\n",
       " 'cell',\n",
       " 'auxiliary',\n",
       " 'an',\n",
       " 'through',\n",
       " 'route',\n",
       " 'disynaptic',\n",
       " 'one',\n",
       " 'and',\n",
       " 'plasticity',\n",
       " 'dependant',\n",
       " 'spike-timing',\n",
       " 'to',\n",
       " 'subject',\n",
       " 'route',\n",
       " 'monosynaptic',\n",
       " 'one',\n",
       " 'routes:',\n",
       " 'two',\n",
       " 'via',\n",
       " 'connected',\n",
       " 'cells',\n",
       " 'spiking',\n",
       " 'simple',\n",
       " 'two',\n",
       " 'of',\n",
       " 'consists',\n",
       " 'circuit',\n",
       " 'this',\n",
       " 'form,',\n",
       " 'canonical',\n",
       " 'its',\n",
       " 'In',\n",
       " 'inputs.',\n",
       " 'known',\n",
       " 'and',\n",
       " 'novel',\n",
       " 'between',\n",
       " 'differentiating',\n",
       " 'explicitly',\n",
       " 'of',\n",
       " 'methods',\n",
       " 'explore',\n",
       " 'to',\n",
       " 'microcircuit',\n",
       " 'neural',\n",
       " 'spiking',\n",
       " 'a',\n",
       " 'developed',\n",
       " 'have',\n",
       " 'We',\n",
       " 'network.',\n",
       " 'the',\n",
       " 'of',\n",
       " 'property',\n",
       " 'intrinsic',\n",
       " 'an',\n",
       " 'is',\n",
       " 'threshold',\n",
       " 'ideal',\n",
       " 'innate',\n",
       " 'some',\n",
       " 'that',\n",
       " 'assumed',\n",
       " 'have',\n",
       " 'proposals',\n",
       " 'these',\n",
       " 'inputs,',\n",
       " 'known',\n",
       " 'on',\n",
       " 'pattern-completion',\n",
       " 'performing',\n",
       " 'network',\n",
       " 'autoassociative',\n",
       " 'an',\n",
       " 'as',\n",
       " 'proposed',\n",
       " 'often',\n",
       " 'is',\n",
       " 'network',\n",
       " 'CA3',\n",
       " 'hippocampal',\n",
       " 'the',\n",
       " 'Although',\n",
       " 'hand.',\n",
       " 'at',\n",
       " 'task',\n",
       " 'the',\n",
       " 'on',\n",
       " 'based',\n",
       " 'vary',\n",
       " 'should',\n",
       " 'distinction',\n",
       " 'this',\n",
       " 'for',\n",
       " 'threshold',\n",
       " 'the',\n",
       " 'that',\n",
       " 'sense',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'Intuitively,',\n",
       " 'novel?',\n",
       " 'is',\n",
       " 'that',\n",
       " 'one',\n",
       " 'and',\n",
       " 'before,',\n",
       " 'experienced',\n",
       " 'has',\n",
       " 'it',\n",
       " 'something',\n",
       " 'to',\n",
       " 'enough’',\n",
       " '’close',\n",
       " 'is',\n",
       " 'that',\n",
       " 'stimulus',\n",
       " 'sensory',\n",
       " 'a',\n",
       " 'between',\n",
       " 'distinguish',\n",
       " 'animal',\n",
       " 'an',\n",
       " 'does',\n",
       " 'how',\n",
       " 'requirements:',\n",
       " 'competing',\n",
       " 'of',\n",
       " 'problem',\n",
       " 'a',\n",
       " 'present',\n",
       " 'cells',\n",
       " 'place',\n",
       " 'of',\n",
       " 'characteristics',\n",
       " 'aforementioned',\n",
       " 'The',\n",
       " 'environment.',\n",
       " 'known',\n",
       " 'a',\n",
       " 'in',\n",
       " 'variation',\n",
       " 'to',\n",
       " 'robust',\n",
       " 'yet',\n",
       " 'learning',\n",
       " 'during',\n",
       " 'discriminatory',\n",
       " 'highly',\n",
       " 'both',\n",
       " 'be',\n",
       " 'can',\n",
       " 'cells',\n",
       " 'the',\n",
       " 'cues,',\n",
       " 'allothetic',\n",
       " 'to',\n",
       " 'respect',\n",
       " 'with',\n",
       " 'is,',\n",
       " 'That',\n",
       " 'removal.',\n",
       " 'cue',\n",
       " 'significant',\n",
       " 'during',\n",
       " 'even',\n",
       " 'stable',\n",
       " 'remain',\n",
       " 'can',\n",
       " 'fields',\n",
       " 'place',\n",
       " 'however,',\n",
       " 'learned',\n",
       " 'Once',\n",
       " 'cues.',\n",
       " 'sensory',\n",
       " 'of',\n",
       " 'number',\n",
       " 'large',\n",
       " 'a',\n",
       " 'sharing',\n",
       " 'potentially',\n",
       " 'despite',\n",
       " 'space,',\n",
       " 'whole',\n",
       " 'the',\n",
       " 'cover',\n",
       " 'together',\n",
       " 'but',\n",
       " 'space,',\n",
       " 'the',\n",
       " 'of',\n",
       " 'fraction',\n",
       " '\\x0cI-57',\n",
       " '',\n",
       " '83',\n",
       " '',\n",
       " '10',\n",
       " 'COSYNE',\n",
       " '',\n",
       " 'a',\n",
       " 'only',\n",
       " 'select',\n",
       " 'individually',\n",
       " 'that',\n",
       " 'fields',\n",
       " 'develop',\n",
       " 'cells',\n",
       " 'these',\n",
       " 'environment,',\n",
       " 'single',\n",
       " 'a',\n",
       " 'Within',\n",
       " 'environment.',\n",
       " 'unknown',\n",
       " 'an',\n",
       " 'of',\n",
       " 'exploration',\n",
       " 'initial',\n",
       " 'during',\n",
       " 'selectivity',\n",
       " 'spatial',\n",
       " 'characteristic',\n",
       " 'their',\n",
       " 'acquire',\n",
       " 'hippocampus',\n",
       " 'rodent',\n",
       " 'the',\n",
       " 'in',\n",
       " 'cells',\n",
       " 'Place',\n",
       " 'Queensland',\n",
       " 'of',\n",
       " 'University',\n",
       " 'The',\n",
       " 'Institute,',\n",
       " 'Brain',\n",
       " 'Queensland',\n",
       " '&',\n",
       " 'ITEE',\n",
       " 'of',\n",
       " 'School',\n",
       " '',\n",
       " 'AU',\n",
       " 'EDU.',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'ITEE',\n",
       " '@',\n",
       " 'JANETW',\n",
       " 'AU',\n",
       " 'EDU.',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'ITEE',\n",
       " '@',\n",
       " 'MILFORD',\n",
       " 'AU',\n",
       " 'EDU.',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'ITEE',\n",
       " '@',\n",
       " 'WYETH',\n",
       " 'AU',\n",
       " 'EDU.',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'ITEE',\n",
       " '@',\n",
       " 'CNOLAN',\n",
       " '',\n",
       " 'Wiles',\n",
       " 'Janet',\n",
       " 'Milford',\n",
       " 'Michael',\n",
       " 'Wyeth',\n",
       " 'Gordon',\n",
       " 'Nolan',\n",
       " 'R.',\n",
       " 'Christopher',\n",
       " 'detection',\n",
       " 'novelty',\n",
       " 'for',\n",
       " 'timing',\n",
       " 'spike',\n",
       " 'using',\n",
       " 'microcircuit',\n",
       " 'neural',\n",
       " 'A',\n",
       " 'I-56.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ehh)):\n",
    "    if ehh[i]=='University' or ehh[i]=='College':\n",
    "        ehh_list.append(ehh[:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  '',\n",
       "  'doi:',\n",
       "  'cells.',\n",
       "  'place',\n",
       "  'real',\n",
       "  'of',\n",
       "  'selectivity',\n",
       "  'and',\n",
       "  'robustness',\n",
       "  'the',\n",
       "  'mimicking',\n",
       "  'responses,',\n",
       "  'new',\n",
       "  'acquiring',\n",
       "  'avoid',\n",
       "  'and',\n",
       "  'representations',\n",
       "  'encoded',\n",
       "  'previously',\n",
       "  'reactivate',\n",
       "  'will',\n",
       "  'environment',\n",
       "  'learned',\n",
       "  'previously',\n",
       "  'a',\n",
       "  'through',\n",
       "  'traversing',\n",
       "  'whether',\n",
       "  'test',\n",
       "  'to',\n",
       "  'is',\n",
       "  'goal',\n",
       "  'Our',\n",
       "  'cues.',\n",
       "  'visual',\n",
       "  'certain',\n",
       "  'to',\n",
       "  'preferentially',\n",
       "  'respond',\n",
       "  'to',\n",
       "  'tuned',\n",
       "  'cells',\n",
       "  'and',\n",
       "  'cells',\n",
       "  'grid',\n",
       "  'mimicking',\n",
       "  'units',\n",
       "  'spiking',\n",
       "  'into',\n",
       "  'processed',\n",
       "  'is',\n",
       "  'which',\n",
       "  'footage,',\n",
       "  'video',\n",
       "  'from',\n",
       "  'derived',\n",
       "  'data',\n",
       "  'real-world',\n",
       "  'with',\n",
       "  'model',\n",
       "  'this',\n",
       "  'testing',\n",
       "  'currently',\n",
       "  'are',\n",
       "  'We',\n",
       "  'basis.',\n",
       "  'pattern-by-pattern',\n",
       "  'a',\n",
       "  'on',\n",
       "  'DG',\n",
       "  'and',\n",
       "  'CA3',\n",
       "  'between',\n",
       "  'timing',\n",
       "  'spike',\n",
       "  'relative',\n",
       "  'of',\n",
       "  'virtue',\n",
       "  'by',\n",
       "  'known',\n",
       "  'or',\n",
       "  'novel',\n",
       "  'as',\n",
       "  'recognized',\n",
       "  'be',\n",
       "  'can',\n",
       "  'patterns',\n",
       "  'input',\n",
       "  'interneurons)',\n",
       "  'inhibitory',\n",
       "  'some',\n",
       "  'of',\n",
       "  'addition',\n",
       "  'the',\n",
       "  '(with',\n",
       "  'then',\n",
       "  'group,',\n",
       "  'cell',\n",
       "  'a',\n",
       "  'instead',\n",
       "  'is',\n",
       "  'microcircuit',\n",
       "  'the',\n",
       "  'in',\n",
       "  'cells',\n",
       "  'three',\n",
       "  'the',\n",
       "  'of',\n",
       "  'each',\n",
       "  'if',\n",
       "  'that',\n",
       "  'demonstrate',\n",
       "  'We',\n",
       "  'hippocampus.',\n",
       "  'the',\n",
       "  'of',\n",
       "  'CA3',\n",
       "  'subregion',\n",
       "  'and',\n",
       "  '(DG)',\n",
       "  'gyrus',\n",
       "  'dentate',\n",
       "  'the',\n",
       "  '(EC),',\n",
       "  'cortex',\n",
       "  'entorhinal',\n",
       "  'the',\n",
       "  'of',\n",
       "  'that',\n",
       "  'to',\n",
       "  'similar',\n",
       "  'is',\n",
       "  'microcircuit',\n",
       "  'this',\n",
       "  'of',\n",
       "  'structure',\n",
       "  'The',\n",
       "  'input.',\n",
       "  'the',\n",
       "  'of',\n",
       "  'familiarity',\n",
       "  'the',\n",
       "  'signals',\n",
       "  '-',\n",
       "  'cell',\n",
       "  'auxiliary',\n",
       "  'the',\n",
       "  'and',\n",
       "  'cell',\n",
       "  'target',\n",
       "  'the',\n",
       "  'between',\n",
       "  'timing',\n",
       "  'spike',\n",
       "  'relative',\n",
       "  'the',\n",
       "  '-',\n",
       "  'race',\n",
       "  'this',\n",
       "  'of',\n",
       "  'result',\n",
       "  'The',\n",
       "  'pathway.',\n",
       "  'disynaptic',\n",
       "  'the',\n",
       "  'via',\n",
       "  'cell',\n",
       "  'same',\n",
       "  'the',\n",
       "  'of',\n",
       "  'activation',\n",
       "  'the',\n",
       "  'and',\n",
       "  'pathway',\n",
       "  'monosynaptic',\n",
       "  'the',\n",
       "  'via',\n",
       "  'cell',\n",
       "  'target',\n",
       "  'the',\n",
       "  'of',\n",
       "  'activation',\n",
       "  'between',\n",
       "  'exists',\n",
       "  'race',\n",
       "  'a',\n",
       "  'network,',\n",
       "  'this',\n",
       "  'in',\n",
       "  'that',\n",
       "  'demonstrate',\n",
       "  'We',\n",
       "  'synapses.',\n",
       "  'fixed-weight',\n",
       "  'with',\n",
       "  'cell',\n",
       "  'auxiliary',\n",
       "  'an',\n",
       "  'through',\n",
       "  'route',\n",
       "  'disynaptic',\n",
       "  'one',\n",
       "  'and',\n",
       "  'plasticity',\n",
       "  'dependant',\n",
       "  'spike-timing',\n",
       "  'to',\n",
       "  'subject',\n",
       "  'route',\n",
       "  'monosynaptic',\n",
       "  'one',\n",
       "  'routes:',\n",
       "  'two',\n",
       "  'via',\n",
       "  'connected',\n",
       "  'cells',\n",
       "  'spiking',\n",
       "  'simple',\n",
       "  'two',\n",
       "  'of',\n",
       "  'consists',\n",
       "  'circuit',\n",
       "  'this',\n",
       "  'form,',\n",
       "  'canonical',\n",
       "  'its',\n",
       "  'In',\n",
       "  'inputs.',\n",
       "  'known',\n",
       "  'and',\n",
       "  'novel',\n",
       "  'between',\n",
       "  'differentiating',\n",
       "  'explicitly',\n",
       "  'of',\n",
       "  'methods',\n",
       "  'explore',\n",
       "  'to',\n",
       "  'microcircuit',\n",
       "  'neural',\n",
       "  'spiking',\n",
       "  'a',\n",
       "  'developed',\n",
       "  'have',\n",
       "  'We',\n",
       "  'network.',\n",
       "  'the',\n",
       "  'of',\n",
       "  'property',\n",
       "  'intrinsic',\n",
       "  'an',\n",
       "  'is',\n",
       "  'threshold',\n",
       "  'ideal',\n",
       "  'innate',\n",
       "  'some',\n",
       "  'that',\n",
       "  'assumed',\n",
       "  'have',\n",
       "  'proposals',\n",
       "  'these',\n",
       "  'inputs,',\n",
       "  'known',\n",
       "  'on',\n",
       "  'pattern-completion',\n",
       "  'performing',\n",
       "  'network',\n",
       "  'autoassociative',\n",
       "  'an',\n",
       "  'as',\n",
       "  'proposed',\n",
       "  'often',\n",
       "  'is',\n",
       "  'network',\n",
       "  'CA3',\n",
       "  'hippocampal',\n",
       "  'the',\n",
       "  'Although',\n",
       "  'hand.',\n",
       "  'at',\n",
       "  'task',\n",
       "  'the',\n",
       "  'on',\n",
       "  'based',\n",
       "  'vary',\n",
       "  'should',\n",
       "  'distinction',\n",
       "  'this',\n",
       "  'for',\n",
       "  'threshold',\n",
       "  'the',\n",
       "  'that',\n",
       "  'sense',\n",
       "  'makes',\n",
       "  'it',\n",
       "  'Intuitively,',\n",
       "  'novel?',\n",
       "  'is',\n",
       "  'that',\n",
       "  'one',\n",
       "  'and',\n",
       "  'before,',\n",
       "  'experienced',\n",
       "  'has',\n",
       "  'it',\n",
       "  'something',\n",
       "  'to',\n",
       "  'enough’',\n",
       "  '’close',\n",
       "  'is',\n",
       "  'that',\n",
       "  'stimulus',\n",
       "  'sensory',\n",
       "  'a',\n",
       "  'between',\n",
       "  'distinguish',\n",
       "  'animal',\n",
       "  'an',\n",
       "  'does',\n",
       "  'how',\n",
       "  'requirements:',\n",
       "  'competing',\n",
       "  'of',\n",
       "  'problem',\n",
       "  'a',\n",
       "  'present',\n",
       "  'cells',\n",
       "  'place',\n",
       "  'of',\n",
       "  'characteristics',\n",
       "  'aforementioned',\n",
       "  'The',\n",
       "  'environment.',\n",
       "  'known',\n",
       "  'a',\n",
       "  'in',\n",
       "  'variation',\n",
       "  'to',\n",
       "  'robust',\n",
       "  'yet',\n",
       "  'learning',\n",
       "  'during',\n",
       "  'discriminatory',\n",
       "  'highly',\n",
       "  'both',\n",
       "  'be',\n",
       "  'can',\n",
       "  'cells',\n",
       "  'the',\n",
       "  'cues,',\n",
       "  'allothetic',\n",
       "  'to',\n",
       "  'respect',\n",
       "  'with',\n",
       "  'is,',\n",
       "  'That',\n",
       "  'removal.',\n",
       "  'cue',\n",
       "  'significant',\n",
       "  'during',\n",
       "  'even',\n",
       "  'stable',\n",
       "  'remain',\n",
       "  'can',\n",
       "  'fields',\n",
       "  'place',\n",
       "  'however,',\n",
       "  'learned',\n",
       "  'Once',\n",
       "  'cues.',\n",
       "  'sensory',\n",
       "  'of',\n",
       "  'number',\n",
       "  'large',\n",
       "  'a',\n",
       "  'sharing',\n",
       "  'potentially',\n",
       "  'despite',\n",
       "  'space,',\n",
       "  'whole',\n",
       "  'the',\n",
       "  'cover',\n",
       "  'together',\n",
       "  'but',\n",
       "  'space,',\n",
       "  'the',\n",
       "  'of',\n",
       "  'fraction',\n",
       "  '\\x0cI-57',\n",
       "  '',\n",
       "  '83',\n",
       "  '',\n",
       "  '10',\n",
       "  'COSYNE',\n",
       "  '',\n",
       "  'a',\n",
       "  'only',\n",
       "  'select',\n",
       "  'individually',\n",
       "  'that',\n",
       "  'fields',\n",
       "  'develop',\n",
       "  'cells',\n",
       "  'these',\n",
       "  'environment,',\n",
       "  'single',\n",
       "  'a',\n",
       "  'Within',\n",
       "  'environment.',\n",
       "  'unknown',\n",
       "  'an',\n",
       "  'of',\n",
       "  'exploration',\n",
       "  'initial',\n",
       "  'during',\n",
       "  'selectivity',\n",
       "  'spatial',\n",
       "  'characteristic',\n",
       "  'their',\n",
       "  'acquire',\n",
       "  'hippocampus',\n",
       "  'rodent',\n",
       "  'the',\n",
       "  'in',\n",
       "  'cells',\n",
       "  'Place',\n",
       "  'Queensland',\n",
       "  'of']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehh_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-56.',\n",
       " 'A',\n",
       " 'neural',\n",
       " 'microcircuit',\n",
       " 'using',\n",
       " 'spike',\n",
       " 'timing',\n",
       " 'for',\n",
       " 'novelty',\n",
       " 'detection',\n",
       " 'Christopher',\n",
       " 'R.',\n",
       " 'Nolan',\n",
       " 'Gordon',\n",
       " 'Wyeth',\n",
       " 'Michael',\n",
       " 'Milford',\n",
       " 'Janet',\n",
       " 'Wiles',\n",
       " '',\n",
       " 'CNOLAN',\n",
       " '@',\n",
       " 'ITEE',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'EDU.',\n",
       " 'AU',\n",
       " 'WYETH',\n",
       " '@',\n",
       " 'ITEE',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'EDU.',\n",
       " 'AU',\n",
       " 'MILFORD',\n",
       " '@',\n",
       " 'ITEE',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'EDU.',\n",
       " 'AU',\n",
       " 'JANETW',\n",
       " '@',\n",
       " 'ITEE',\n",
       " '.',\n",
       " 'UQ',\n",
       " '.',\n",
       " 'EDU.',\n",
       " 'AU',\n",
       " '',\n",
       " 'School',\n",
       " 'of',\n",
       " 'ITEE',\n",
       " '&',\n",
       " 'Queensland',\n",
       " 'Brain',\n",
       " 'Institute,',\n",
       " 'The',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Queensland',\n",
       " 'Place',\n",
       " 'cells',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rodent',\n",
       " 'hippocampus',\n",
       " 'acquire',\n",
       " 'their',\n",
       " 'characteristic',\n",
       " 'spatial',\n",
       " 'selectivity',\n",
       " 'during',\n",
       " 'initial',\n",
       " 'exploration',\n",
       " 'of',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'environment.',\n",
       " 'Within',\n",
       " 'a',\n",
       " 'single',\n",
       " 'environment,',\n",
       " 'these',\n",
       " 'cells',\n",
       " 'develop',\n",
       " 'fields',\n",
       " 'that',\n",
       " 'individually',\n",
       " 'select',\n",
       " 'only',\n",
       " 'a',\n",
       " '',\n",
       " 'COSYNE',\n",
       " '10',\n",
       " '',\n",
       " '83',\n",
       " '',\n",
       " '\\x0cI-57',\n",
       " 'fraction',\n",
       " 'of',\n",
       " 'the',\n",
       " 'space,',\n",
       " 'but',\n",
       " 'together',\n",
       " 'cover',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'space,',\n",
       " 'despite',\n",
       " 'potentially',\n",
       " 'sharing',\n",
       " 'a',\n",
       " 'large',\n",
       " 'number',\n",
       " 'of',\n",
       " 'sensory',\n",
       " 'cues.',\n",
       " 'Once',\n",
       " 'learned',\n",
       " 'however,',\n",
       " 'place',\n",
       " 'fields',\n",
       " 'can',\n",
       " 'remain',\n",
       " 'stable',\n",
       " 'even',\n",
       " 'during',\n",
       " 'significant',\n",
       " 'cue',\n",
       " 'removal.',\n",
       " 'That',\n",
       " 'is,',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'allothetic',\n",
       " 'cues,',\n",
       " 'the',\n",
       " 'cells',\n",
       " 'can',\n",
       " 'be',\n",
       " 'both',\n",
       " 'highly',\n",
       " 'discriminatory',\n",
       " 'during',\n",
       " 'learning',\n",
       " 'yet',\n",
       " 'robust',\n",
       " 'to',\n",
       " 'variation',\n",
       " 'in',\n",
       " 'a',\n",
       " 'known',\n",
       " 'environment.',\n",
       " 'The',\n",
       " 'aforementioned',\n",
       " 'characteristics',\n",
       " 'of',\n",
       " 'place',\n",
       " 'cells',\n",
       " 'present',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'competing',\n",
       " 'requirements:',\n",
       " 'how',\n",
       " 'does',\n",
       " 'an',\n",
       " 'animal',\n",
       " 'distinguish',\n",
       " 'between',\n",
       " 'a',\n",
       " 'sensory',\n",
       " 'stimulus',\n",
       " 'that',\n",
       " 'is',\n",
       " '’close',\n",
       " 'enough’',\n",
       " 'to',\n",
       " 'something',\n",
       " 'it',\n",
       " 'has',\n",
       " 'experienced',\n",
       " 'before,',\n",
       " 'and',\n",
       " 'one',\n",
       " 'that',\n",
       " 'is',\n",
       " 'novel?',\n",
       " 'Intuitively,',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'sense',\n",
       " 'that',\n",
       " 'the',\n",
       " 'threshold',\n",
       " 'for',\n",
       " 'this',\n",
       " 'distinction',\n",
       " 'should',\n",
       " 'vary',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'task',\n",
       " 'at',\n",
       " 'hand.',\n",
       " 'Although',\n",
       " 'the',\n",
       " 'hippocampal',\n",
       " 'CA3',\n",
       " 'network',\n",
       " 'is',\n",
       " 'often',\n",
       " 'proposed',\n",
       " 'as',\n",
       " 'an',\n",
       " 'autoassociative',\n",
       " 'network',\n",
       " 'performing',\n",
       " 'pattern-completion',\n",
       " 'on',\n",
       " 'known',\n",
       " 'inputs,',\n",
       " 'these',\n",
       " 'proposals',\n",
       " 'have',\n",
       " 'assumed',\n",
       " 'that',\n",
       " 'some',\n",
       " 'innate',\n",
       " 'ideal',\n",
       " 'threshold',\n",
       " 'is',\n",
       " 'an',\n",
       " 'intrinsic',\n",
       " 'property',\n",
       " 'of',\n",
       " 'the',\n",
       " 'network.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'spiking',\n",
       " 'neural',\n",
       " 'microcircuit',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'methods',\n",
       " 'of',\n",
       " 'explicitly',\n",
       " 'differentiating',\n",
       " 'between',\n",
       " 'novel',\n",
       " 'and',\n",
       " 'known',\n",
       " 'inputs.',\n",
       " 'In',\n",
       " 'its',\n",
       " 'canonical',\n",
       " 'form,',\n",
       " 'this',\n",
       " 'circuit',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'two',\n",
       " 'simple',\n",
       " 'spiking',\n",
       " 'cells',\n",
       " 'connected',\n",
       " 'via',\n",
       " 'two',\n",
       " 'routes:',\n",
       " 'one',\n",
       " 'monosynaptic',\n",
       " 'route',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'spike-timing',\n",
       " 'dependant',\n",
       " 'plasticity',\n",
       " 'and',\n",
       " 'one',\n",
       " 'disynaptic',\n",
       " 'route',\n",
       " 'through',\n",
       " 'an',\n",
       " 'auxiliary',\n",
       " 'cell',\n",
       " 'with',\n",
       " 'fixed-weight',\n",
       " 'synapses.',\n",
       " 'We',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'in',\n",
       " 'this',\n",
       " 'network,',\n",
       " 'a',\n",
       " 'race',\n",
       " 'exists',\n",
       " 'between',\n",
       " 'activation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'target',\n",
       " 'cell',\n",
       " 'via',\n",
       " 'the',\n",
       " 'monosynaptic',\n",
       " 'pathway',\n",
       " 'and',\n",
       " 'the',\n",
       " 'activation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'cell',\n",
       " 'via',\n",
       " 'the',\n",
       " 'disynaptic',\n",
       " 'pathway.',\n",
       " 'The',\n",
       " 'result',\n",
       " 'of',\n",
       " 'this',\n",
       " 'race',\n",
       " '-',\n",
       " 'the',\n",
       " 'relative',\n",
       " 'spike',\n",
       " 'timing',\n",
       " 'between',\n",
       " 'the',\n",
       " 'target',\n",
       " 'cell',\n",
       " 'and',\n",
       " 'the',\n",
       " 'auxiliary',\n",
       " 'cell',\n",
       " '-',\n",
       " 'signals',\n",
       " 'the',\n",
       " 'familiarity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'input.',\n",
       " 'The',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'this',\n",
       " 'microcircuit',\n",
       " 'is',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'that',\n",
       " 'of',\n",
       " 'the',\n",
       " 'entorhinal',\n",
       " 'cortex',\n",
       " '(EC),',\n",
       " 'the',\n",
       " 'dentate',\n",
       " 'gyrus',\n",
       " '(DG)',\n",
       " 'and',\n",
       " 'subregion',\n",
       " 'CA3',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hippocampus.',\n",
       " 'We',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'if',\n",
       " 'each',\n",
       " 'of',\n",
       " 'the',\n",
       " 'three',\n",
       " 'cells',\n",
       " 'in',\n",
       " 'the',\n",
       " 'microcircuit',\n",
       " 'is',\n",
       " 'instead',\n",
       " 'a',\n",
       " 'cell',\n",
       " 'group,',\n",
       " 'then',\n",
       " '(with',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'some',\n",
       " 'inhibitory',\n",
       " 'interneurons)',\n",
       " 'input',\n",
       " 'patterns',\n",
       " 'can',\n",
       " 'be',\n",
       " 'recognized',\n",
       " 'as',\n",
       " 'novel',\n",
       " 'or',\n",
       " 'known',\n",
       " 'by',\n",
       " 'virtue',\n",
       " 'of',\n",
       " 'relative',\n",
       " 'spike',\n",
       " 'timing',\n",
       " 'between',\n",
       " 'CA3',\n",
       " 'and',\n",
       " 'DG',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pattern-by-pattern',\n",
       " 'basis.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'currently',\n",
       " 'testing',\n",
       " 'this',\n",
       " 'model',\n",
       " 'with',\n",
       " 'real-world',\n",
       " 'data',\n",
       " 'derived',\n",
       " 'from',\n",
       " 'video',\n",
       " 'footage,',\n",
       " 'which',\n",
       " 'is',\n",
       " 'processed',\n",
       " 'into',\n",
       " 'spiking',\n",
       " 'units',\n",
       " 'mimicking',\n",
       " 'grid',\n",
       " 'cells',\n",
       " 'and',\n",
       " 'cells',\n",
       " 'tuned',\n",
       " 'to',\n",
       " 'respond',\n",
       " 'preferentially',\n",
       " 'to',\n",
       " 'certain',\n",
       " 'visual',\n",
       " 'cues.',\n",
       " 'Our',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'to',\n",
       " 'test',\n",
       " 'whether',\n",
       " 'traversing',\n",
       " 'through',\n",
       " 'a',\n",
       " 'previously',\n",
       " 'learned',\n",
       " 'environment',\n",
       " 'will',\n",
       " 'reactivate',\n",
       " 'previously',\n",
       " 'encoded',\n",
       " 'representations',\n",
       " 'and',\n",
       " 'avoid',\n",
       " 'acquiring',\n",
       " 'new',\n",
       " 'responses,',\n",
       " 'mimicking',\n",
       " 'the',\n",
       " 'robustness',\n",
       " 'and',\n",
       " 'selectivity',\n",
       " 'of',\n",
       " 'real',\n",
       " 'place',\n",
       " 'cells.',\n",
       " 'doi:',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idea being after a few capitalized words start collecting abstracts\n",
    "first you would have to get past the title sentence possibly by matching that with the table of contents\n",
    "\n",
    "then you might be able to implement checking the cap words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the abstracts\n",
    "- wont be a full clean but maybe can clean from the bottom up so the last thing that is in the abstract to the top and stopping at the top once the string \"college\" or \"university\" was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9e71dcb6cfca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mabs_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'T-36 – I-1'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mstart_sect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'I-1.'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "CNS_file=[]\n",
    "start_sect=[]\n",
    "beg_ind=[]\n",
    "end_ind=[]\n",
    "\n",
    "file = open(data_folder+CNS_files[2], 'r')\n",
    "data = file.read()\n",
    "data_list = data.split('\\n')\n",
    "abs_start = [ind for ind, d in enumerate(data_list) if 'T-36 – I-1' in d][0]\n",
    "start_sect.append([ind for ind, d in enumerate(data_list) if 'I-1.' in d])\n",
    "\n",
    "abs_list = data_list[abs_start:]\n",
    "\n",
    "\n",
    "mush=''.join(abs_list)\n",
    "\n",
    "cur_section = 'I-'\n",
    "add_dot = '.'\n",
    "#cur_section = 'E '\n",
    "cur_abs = 1\n",
    "abs_beg_ind = mush.index(cur_section+'%i'%cur_abs+add_dot)\n",
    "abs_end_ind = mush.index(cur_section+'%i'%(cur_abs+1)+add_dot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(data_folder+CNS_files[2], 'r')\n",
    "data = file.read()\n",
    "data_list = data.split('\\n')\n",
    "abs_start = [ind for ind, d in enumerate(data_list) if 'T-36 – I-1' in d][0]\n",
    "\n",
    "\n",
    "abs_list = data_list[abs_start:]\n",
    "\n",
    "\n",
    "mush=''.join(abs_list)\n",
    "\n",
    "abstracts=[]\n",
    "let_vec = ['I-','II-','III-']\n",
    "for j in range(0,len(let_vec)):\n",
    "\n",
    "    print(j)\n",
    "\n",
    "    for i in range(0,200):\n",
    "\n",
    "        try:\n",
    "            cur_section= let_vec[j]\n",
    "            add_dot = '.'\n",
    "            cur_abs=i\n",
    "            abs_beg_ind = mush.index(cur_section+'%i'%cur_abs+add_dot)\n",
    "            abs_end_ind = mush.index(cur_section+'%i'%(cur_abs+1)+add_dot)\n",
    "            last_index = cur_section+'%i'%(cur_abs+1)+add_dot\n",
    "            print(cur_abs)\n",
    "\n",
    "            section_abst = mush[abs_beg_ind:abs_end_ind]\n",
    "            \n",
    "            abstracts.append(section_abst)\n",
    "            \n",
    "            #last_good = i\n",
    "        except:\n",
    "            'ur mom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maybe something like if the last 4 words were capitalized then dont start the abstract but when there is one \n",
    "capitalized word and then the preceeding 3 words are not caps then start the abstract?\n",
    "\n",
    "Is that too much of a long shot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-10fecffd78ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mabs_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'Poster session I-1'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mstart_sect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'Poster session I-1'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#this is a start to 2008/2009 \n",
    "# it looks like 2010-2018 are more consistent in formatting\n",
    "CNS_file=[]\n",
    "start_sect=[]\n",
    "beg_ind=[]\n",
    "end_ind=[]\n",
    "\n",
    "file = open(data_folder+CNS_files[0], 'r')\n",
    "data = file.read()\n",
    "data_list = data.split('\\n')\n",
    "abs_start = [ind for ind, d in enumerate(data_list) if 'Poster session I-1' in d][0]\n",
    "start_sect.append([ind for ind, d in enumerate(data_list) if 'Poster session I-1' in d])\n",
    "\n",
    "abs_list = data_list[abs_start:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abs_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In running the below cell i see that i need to index using the whole thursday thing\n",
    "\n",
    "sooooooo this is only for 2008 so far so the format could be different for the rest of the years\n",
    "\n",
    "was thinking for 2008 could split on commas then i could actually have poster session I-# \n",
    "and in doing this it could potentially work for the rest of the years assuming that they all work \n",
    "like this as well\n",
    "\n",
    "butttttt i think there is a good starting frame work and then i can potentially take the solves for issues within CNS\n",
    "and apply them in slightly different ways to this set of abstracts bc i realize there will be weird little things\n",
    "in all of these sets especially moving to a complete different journal thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_list[618:630] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abs_list.index('Poster session I-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this cell is the bare bones for how the function will run\n",
    "\n",
    "\n",
    "cur_section = 'Poster session I-'\n",
    "#cur_section = 'E '\n",
    "cur_abs = 1\n",
    "abs_beg_ind = abs_list.index(cur_section+'%i'%cur_abs)\n",
    "abs_end_ind = abs_list.index(cur_section+'%i'%(cur_abs+1))\n",
    "\n",
    "section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "start_abst=section_abst.index(\" — \")#index with in the section where we first see this character - which denotes start of abs\n",
    "whole_abs=section_abst[start_abst:len(section_abst)]#the abstract separated from the title and author\n",
    "title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "\n",
    "\n",
    "length_title_lst=' '.join(title_lst)\n",
    "\n",
    "auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#col_names = ['CNS_file','start_sect','beg_ind','end_ind']\n",
    "\n",
    "#reffering to the abs_list problem encountered in journal on 8/2/2018. I think this needs to be on the outside of the loop so that it can catch each abs_list year change\n",
    "\n",
    "\n",
    "CNS_file=[]\n",
    "start_sect=[]\n",
    "beg_ind=[]\n",
    "end_ind=[]\n",
    "\n",
    "for i in range(len(CNS_files)):\n",
    "    \n",
    "    CNS_file.append(CNS_files[i])\n",
    "    \n",
    "    #print(CNS_files[i])\n",
    "    file = open(data_folder+CNS_files[i], 'r')\n",
    "    data = file.read()\n",
    "    data_list = data.split('\\n')\n",
    "    abs_start = [ind for ind, d in enumerate(data_list) if '\\x0cPoster session I-1' in d][0]#tells you the ind where the first abstract is?\n",
    "    \n",
    "#     start_sect.append([ind for ind, d in enumerate(data_list) if '\\x0cPoster session I-1' in d])\n",
    "#     #print([ind for ind, d in enumerate(data_list) if '\\x0cPoster Session A' in d])\n",
    "    \n",
    "#     abs_list = data_list[abs_start:]\n",
    "#     poster_beg_ind = next((ind for ind,s in enumerate(abs_list) if '\\x0cPoster session I-1' == s), None)    \n",
    "#     poster_end_ind = next((ind for ind,s in enumerate(abs_list) if '\\x0cTalks-1' == s), None)\n",
    "    \n",
    "    \n",
    "#     beg_ind.append(poster_beg_ind)\n",
    "    \n",
    "#     end_ind.append(poster_end_ind)\n",
    "    \n",
    "    \n",
    "#     #print(poster_beg_ind, poster_end_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cns_frame=pd.DataFrame({'CNS_file':CNS_file,'start_sect':start_sect,'beg_ind':beg_ind,'end_ind':end_ind})\n",
    "cns_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the first file is problematic,CNS_2007, until cell 106 (excel) which is when it hits F1, and then everything works perfect\n",
    "\n",
    "We do miss the very last index for 2016 but i think thats the only last index that is missed. Did random checks and this appears true.\n",
    "\n",
    "FIGURED it out! so it looks like the sections in 2007 are messed up until F for numbering the 10's\n",
    "\n",
    "EX:\n",
    "A 10(doesnt work)\n",
    "B 10(doesnt work)\n",
    "...\n",
    "F10(works)\n",
    "\n",
    "and it's all of the numbers until F\n",
    "\n",
    "solutions ideas\n",
    "maybe just run a find and delete B ## and make it B## in the text doc.\n",
    "\n",
    "So will need to write something to grab these? Maybe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sect_abs=[]\n",
    "abst=[]\n",
    "title=[]\n",
    "auth=[]\n",
    "CNS_Year=[]\n",
    "\n",
    "let_vec = ['A','B','C','D','E','F','G','H','I','J']\n",
    "for x in range(len(CNS_files)):\n",
    "\n",
    "    \n",
    "    file = open(data_folder+CNS_files[x], 'r')# looks like this is the place where we call the specific year to parse\n",
    "    data = file.read()\n",
    "    data_list = data.split('\\n')\n",
    "    abs_start = [ind for ind, d in enumerate(data_list) if '\\x0cPoster Session A' in d][0]#tells you the ind where the first abstract is?\n",
    "    abs_list = data_list[abs_start:]\n",
    "    poster_beg_ind = data_list.index('\\x0cPoster Session A')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for j in range(0,len(let_vec)):\n",
    "\n",
    "        #print(j)\n",
    "\n",
    "        for i in range(0,200):\n",
    "\n",
    "            try:\n",
    "                cur_section= let_vec[j]\n",
    "                cur_abs=i\n",
    "                abs_beg_ind = abs_list.index(cur_section+'%i'%cur_abs)\n",
    "                abs_end_ind = abs_list.index(cur_section+'%i'%(cur_abs+1))\n",
    "                last_index = cur_section+'%i'%(cur_abs+1)\n",
    "                #print(cur_abs)\n",
    "\n",
    "                section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                \n",
    "                start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                start_string=cur_section+'%i'%cur_abs #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                length_title_lst=' '.join(title_lst)\n",
    "                remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "\n",
    "                sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "                abst.append(whole_abs)\n",
    "                title.append(remove_start_string)\n",
    "                \n",
    "                auth.append(auth_sect)\n",
    "                CNS_Year.append(CNS_file[x])\n",
    "\n",
    "\n",
    "                last_good = i\n",
    "            except ValueError:\n",
    "\n",
    "                for k in range(2,51):\n",
    "                    try:\n",
    "                        #print('k = ',k)\n",
    "                        #print(cur_section+'%i'%(cur_abs+k))\n",
    "                        abs_end_ind = abs_list.index(cur_section+'%i'%(cur_abs+k))\n",
    "\n",
    "                        section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                        #     print(section_abst)\n",
    "                        #     print(cur_section+'%i'%(cur_abs+1))\n",
    "                        #     print(i, abs_beg_ind, abs_end_ind)\n",
    "                        start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                        start_string=cur_section+'%i'%cur_abs #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                        whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                        title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                        title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                        title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                        length_title_lst=' '.join(title_lst)\n",
    "                        remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                        auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "                        sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "                        abst.append(whole_abs)\n",
    "                        title.append(remove_start_string)\n",
    "                        \n",
    "                        auth.append(auth_sect)\n",
    "                        CNS_Year.append(CNS_file[x])\n",
    "\n",
    "\n",
    "                        break\n",
    "                    except:\n",
    "                        #print('except')\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #print('reached else')\n",
    "                        pass\n",
    "\n",
    "                #print('last index =',last_index)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "            except:\n",
    "\n",
    "                                abs_beg_ind = abs_list.index(last_index)\n",
    "                                nxt = cur_section + 1\n",
    "                                abs_end_ind = abs_list.index(let_vec[j+1]+'%i'%nxt)\n",
    "\n",
    "                                section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                                #     print(section_abst)\n",
    "                                #     print(cur_section+'%i'%(cur_abs+1))\n",
    "                                #     print(i, abs_beg_ind, abs_end_ind)\n",
    "                                start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                                start_string=last_index #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                                whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                                title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                                title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                                title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                                length_title_lst=' '.join(title_lst)\n",
    "                                remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                                auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "                                sect_abs.append(last_index)\n",
    "                                abst.append(whole_abs)\n",
    "                                title.append(remove_start_string)\n",
    "                                #auth.append(title_sect) old wrong way of getting author\n",
    "                                auth.append(auth_sect)\n",
    "                                CNS_Year.append(CNS_file[x])\n",
    "                                last_index\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "    print(last_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works. But has some stuff left in it. The above is the same but cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this cell specifically needs to be copied and pasted for every journal and needs to go first because all the rest of the cells use it to initialze abs_list\n",
    "sect_abs=[]\n",
    "abst=[]\n",
    "title=[]\n",
    "auth=[]\n",
    "CNS_Year=[]\n",
    "i=1\n",
    "let_vec = ['A','B','C','D','E','F','G','H','I']\n",
    "for x in range(len(CNS_files)):\n",
    "    letter_start_ind = data_list.index('\\x0cPoster Session A')\n",
    "    letter_end_ind = data_list.index('\\x0cPoster Session B')\n",
    "    a_b_range= range(letter_start_ind,letter_end_ind)\n",
    "    a_b_range=list(a_b_range)\n",
    "    \n",
    "    file = open(data_folder+CNS_files[x], 'r')# looks like this is the place where we call the specific year to parse\n",
    "    data = file.read()\n",
    "    data_list = data.split('\\n')\n",
    "    abs_start = [ind for ind, d in enumerate(data_list) if '\\x0cPoster Session A' in d][0]#tells you the ind where the first abstract is?\n",
    "    abs_list = data_list[abs_start:]\n",
    "    poster_beg_ind = data_list.index('\\x0cPoster Session A')\n",
    "\n",
    "    z=1\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(0,len(let_vec)):\n",
    "\n",
    "        print(j)\n",
    "\n",
    "        for i in range(0,200):\n",
    "\n",
    "            try:\n",
    "                cur_section= let_vec[j]\n",
    "                cur_abs=i\n",
    "                abs_beg_ind = abs_list.index(cur_section+'%i'%cur_abs)\n",
    "                abs_end_ind = abs_list.index(cur_section+'%i'%(cur_abs+1))\n",
    "                last_index = cur_section+'%i'%(cur_abs+1)\n",
    "                print(cur_abs)\n",
    "\n",
    "                section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                #     print(section_abst)\n",
    "                #     print(cur_section+'%i'%(cur_abs+1))\n",
    "                #     print(i, abs_beg_ind, abs_end_ind)\n",
    "                start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                start_string=cur_section+'%i'%cur_abs #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                length_title_lst=' '.join(title_lst)\n",
    "                remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "\n",
    "                sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "                abst.append(whole_abs)\n",
    "                title.append(remove_start_string)\n",
    "                #auth.append(title_sect) old wrong way of getting author\n",
    "                auth.append(auth_sect)\n",
    "                CNS_Year.append(CNS_file[x])\n",
    "\n",
    "\n",
    "                last_good = i\n",
    "            except ValueError:\n",
    "\n",
    "                for k in range(2,51):\n",
    "                    try:\n",
    "                        #print('k = ',k)\n",
    "                        #print(cur_section+'%i'%(cur_abs+k))\n",
    "                        abs_end_ind = abs_list.index(cur_section+'%i'%(cur_abs+k))\n",
    "\n",
    "                        section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                        #     print(section_abst)\n",
    "                        #     print(cur_section+'%i'%(cur_abs+1))\n",
    "                        #     print(i, abs_beg_ind, abs_end_ind)\n",
    "                        start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                        start_string=cur_section+'%i'%cur_abs #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                        whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                        title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                        title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                        title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                        length_title_lst=' '.join(title_lst)\n",
    "                        remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                        auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "                        sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "                        abst.append(whole_abs)\n",
    "                        title.append(remove_start_string)\n",
    "                        #auth.append(title_sect) old wrong way of getting author\n",
    "                        auth.append(auth_sect)\n",
    "                        CNS_Year.append(CNS_file[x])\n",
    "\n",
    "\n",
    "                        break\n",
    "                    except:\n",
    "                        #print('except')\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #print('reached else')\n",
    "                        pass\n",
    "\n",
    "                #print('last index =',last_index)\n",
    "\n",
    "\n",
    "\n",
    "    #                 except ValueError:\n",
    "    #                             abs_beg_ind = abs_list.index(last_index)\n",
    "    #                             #nxt = cur_section + 1\n",
    "    #                             abs_end_ind = abs_list.index('B'+'%i'%z)\n",
    "\n",
    "    #                             section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "    #                             #     print(section_abst)\n",
    "    #                             #     print(cur_section+'%i'%(cur_abs+1))\n",
    "    #                             #     print(i, abs_beg_ind, abs_end_ind)\n",
    "    #                             start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "    #                             start_string=last_index #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "    #                             whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "    #                             title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "    #                             title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "    #                             title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "    #                             length_title_lst=' '.join(title_lst)\n",
    "    #                             remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "    #                             auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "    #                             sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "    #                             abst.append(whole_abs)\n",
    "    #                             title.append(remove_start_string)\n",
    "    #                             #auth.append(title_sect) old wrong way of getting author\n",
    "    #                             auth.append(auth_sect)\n",
    "    #                             last_index\n",
    "    #                     # letter + last index up to B1 \n",
    "    #                     # section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract\n",
    "\n",
    "    #                             pass\n",
    "            except:\n",
    "\n",
    "                                abs_beg_ind = abs_list.index(last_index)\n",
    "                                #nxt = cur_section + 1\n",
    "                                abs_end_ind = abs_list.index(let_vec[j+1]+'%i'%z)\n",
    "\n",
    "                                section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                                #     print(section_abst)\n",
    "                                #     print(cur_section+'%i'%(cur_abs+1))\n",
    "                                #     print(i, abs_beg_ind, abs_end_ind)\n",
    "                                start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                                start_string=last_index #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                                whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                                title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                                title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                                title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                                length_title_lst=' '.join(title_lst)\n",
    "                                remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                                auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "                                sect_abs.append(last_index)\n",
    "                                abst.append(whole_abs)\n",
    "                                title.append(remove_start_string)\n",
    "                                #auth.append(title_sect) old wrong way of getting author\n",
    "                                auth.append(auth_sect)\n",
    "                                CNS_Year.append(CNS_file[x])\n",
    "                                last_index\n",
    "                        # letter + last index up to B1 \n",
    "                        # section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "    print(last_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making a datatable to see how the data fills the table which will help better understand the format i need to put the data into\n",
    "try_table=pd.DataFrame({'sect_abs':sect_abs,'abst':abst,'title':title,'auth':auth, 'CNS_Year':CNS_Year})#'CNS_Year':CNS_Year\n",
    "try_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try_table.to_csv(\"CNS_SCRAPED_DI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works for one file at a time, the above loops through all the files in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this cell specifically needs to be copied and pasted for every journal and needs to go first because all the rest of the cells use it to initialze abs_list\n",
    "sect_abs=[]\n",
    "abst=[]\n",
    "title=[]\n",
    "auth=[]\n",
    "i=1\n",
    "letter_start_ind = data_list.index('\\x0cPoster Session A')\n",
    "letter_end_ind = data_list.index('\\x0cPoster Session B')\n",
    "a_b_range= range(letter_start_ind,letter_end_ind)\n",
    "a_b_range=list(a_b_range)\n",
    "file = open(data_folder+CNS_files[5], 'r')# looks like this is the place where we call the specific year to parse\n",
    "data = file.read()\n",
    "data_list = data.split('\\n')\n",
    "abs_start = [ind for ind, d in enumerate(data_list) if '\\x0cPoster Session A' in d][0]#tells you the ind where the first abstract is?\n",
    "abs_list = data_list[abs_start:]\n",
    "poster_beg_ind = data_list.index('\\x0cPoster Session A')\n",
    "\n",
    "z=1\n",
    "\n",
    "\n",
    "let_vec = ['A','B','C','D','E','F','G','H','I']\n",
    "\n",
    "for j in range(0,len(let_vec)):\n",
    "    \n",
    "    print(j)\n",
    "\n",
    "    for i in range(0,200):\n",
    "\n",
    "        try:\n",
    "            cur_section= let_vec[j]\n",
    "            cur_abs=i\n",
    "            abs_beg_ind = abs_list.index(cur_section+'%i'%cur_abs)\n",
    "            abs_end_ind = abs_list.index(cur_section+'%i'%(cur_abs+1))\n",
    "            last_index = cur_section+'%i'%(cur_abs+1)\n",
    "            print(cur_abs)\n",
    "\n",
    "            section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "            #     print(section_abst)\n",
    "            #     print(cur_section+'%i'%(cur_abs+1))\n",
    "            #     print(i, abs_beg_ind, abs_end_ind)\n",
    "            start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "            start_string=cur_section+'%i'%cur_abs #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "            whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "            title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "            title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "            title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "            length_title_lst=' '.join(title_lst)\n",
    "            remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "            auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "\n",
    "            sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "            abst.append(whole_abs)\n",
    "            title.append(remove_start_string)\n",
    "            #auth.append(title_sect) old wrong way of getting author\n",
    "            auth.append(auth_sect)\n",
    "            \n",
    "            \n",
    "            last_good = i\n",
    "        except ValueError:\n",
    "            \n",
    "            for k in range(2,51):\n",
    "                try:\n",
    "                    #print('k = ',k)\n",
    "                    #print(cur_section+'%i'%(cur_abs+k))\n",
    "                    abs_end_ind = abs_list.index(cur_section+'%i'%(cur_abs+k))\n",
    "                    \n",
    "                    section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                    #     print(section_abst)\n",
    "                    #     print(cur_section+'%i'%(cur_abs+1))\n",
    "                    #     print(i, abs_beg_ind, abs_end_ind)\n",
    "                    start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                    start_string=cur_section+'%i'%cur_abs #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                    whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                    title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                    title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                    title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                    length_title_lst=' '.join(title_lst)\n",
    "                    remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                    auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "                    sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "                    abst.append(whole_abs)\n",
    "                    title.append(remove_start_string)\n",
    "                    #auth.append(title_sect) old wrong way of getting author\n",
    "                    auth.append(auth_sect)\n",
    "                    \n",
    "                    \n",
    "                    break\n",
    "                except:\n",
    "                    #print('except')\n",
    "                    pass\n",
    "\n",
    "       \n",
    "                else:\n",
    "                    #print('reached else')\n",
    "                    pass\n",
    "\n",
    "            #print('last index =',last_index)\n",
    "\n",
    "\n",
    "\n",
    "#                 except ValueError:\n",
    "#                             abs_beg_ind = abs_list.index(last_index)\n",
    "#                             #nxt = cur_section + 1\n",
    "#                             abs_end_ind = abs_list.index('B'+'%i'%z)\n",
    "\n",
    "#                             section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "#                             #     print(section_abst)\n",
    "#                             #     print(cur_section+'%i'%(cur_abs+1))\n",
    "#                             #     print(i, abs_beg_ind, abs_end_ind)\n",
    "#                             start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "#                             start_string=last_index #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "#                             whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "#                             title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "#                             title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "#                             title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "#                             length_title_lst=' '.join(title_lst)\n",
    "#                             remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "#                             auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "#                             sect_abs.append(cur_section+'%i'%cur_abs)\n",
    "#                             abst.append(whole_abs)\n",
    "#                             title.append(remove_start_string)\n",
    "#                             #auth.append(title_sect) old wrong way of getting author\n",
    "#                             auth.append(auth_sect)\n",
    "#                             last_index\n",
    "#                     # letter + last index up to B1 \n",
    "#                     # section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract\n",
    "\n",
    "#                             pass\n",
    "        except:\n",
    "            \n",
    "                            abs_beg_ind = abs_list.index(last_index)\n",
    "                            #nxt = cur_section + 1\n",
    "                            abs_end_ind = abs_list.index(let_vec[j+1]+'%i'%z)\n",
    "\n",
    "                            section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract \n",
    "                            #     print(section_abst)\n",
    "                            #     print(cur_section+'%i'%(cur_abs+1))\n",
    "                            #     print(i, abs_beg_ind, abs_end_ind)\n",
    "                            start_abst=section_abst.index(' — ')#index with in the section where we first see this character - which denotes start of abs\n",
    "                            start_string=last_index #the F4 like number at the beg of each title that isnt needed\n",
    "\n",
    "                            whole_abs=section_abst[start_abst+3:len(section_abst)]#the abstract separated from the title and author\n",
    "                            title_sect=section_abst[0:start_abst]#the title and author combined\n",
    "                            title_auth_lst=title_sect.split(' ')#splits them up into list items so that the lamdas will work? have not tried to run without this\n",
    "                            title_lst=[word for word in title_auth_lst if word.isupper()]#takes only all uppercase words\n",
    "                            length_title_lst=' '.join(title_lst)\n",
    "                            remove_start_string=length_title_lst.replace(start_string,\"\")\n",
    "                            auth_sect=section_abst[len(length_title_lst):start_abst]\n",
    "\n",
    "\n",
    "                            sect_abs.append(last_index)\n",
    "                            abst.append(whole_abs)\n",
    "                            title.append(remove_start_string)\n",
    "                            #auth.append(title_sect) old wrong way of getting author\n",
    "                            auth.append(auth_sect)\n",
    "                            last_index\n",
    "                    # letter + last index up to B1 \n",
    "                    # section_abst=' '.join(abs_list[abs_beg_ind:abs_end_ind])#entire section including title author and abstract\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "                pass\n",
    "\n",
    "            \n",
    "            \n",
    "print(last_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making a datatable to see how the data fills the table which will help better understand the format i need to put the data into\n",
    "try_table=pd.DataFrame({'sect_abs':sect_abs,'abst':abst,'title':title,'auth':auth})\n",
    "try_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
